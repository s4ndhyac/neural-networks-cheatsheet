{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Bayes Classifiers\n",
    "###### 1. Compute all the probabilities necessary for a naive Bayes classifer, i.e., the class probability <em>p(y)</em> and all the individual feature probabilities <em>p(x<sub>i</sub>|y)</em>, for each class  <em>y</em> and each feature <em>x</em>\n",
    "\n",
    "Given the table of binary valued features and the class y, we can calculate the probabilities necessary for a naive Bayes classifier by merely counting the no. of data points with a given feature value (1 or 0) for a given class (-1 or 1), since we can assume that they are conditionally independent for the purposes of a naive bayes classifier.\n",
    "\n",
    "| i  | P(x<sub>i</sub>=0 &#124; y=1) | P(x<sub>i</sub>=0 &#124; y=-1) | P(x<sub>i</sub>=1 &#124; y=1) | P(x<sub>i</sub>=1 &#124; y=-1) |\n",
    "|---|---|---|---|---|\n",
    "|1|1/4|1/2|3/4|1/2|\n",
    "|2|1|1/6|0|5/6|\n",
    "|3|1/4|1/3|3/4|2/3|\n",
    "|4|1/2|1/6|1/2|5/6|\n",
    "|5|3/4|2/3|1/4|1/3|\n",
    "\n",
    "| P(y=1) | P(y=-1) |\n",
    "|---|---|\n",
    "| 2/5 | 3/5 |\n",
    "\n",
    "###### 2. Which class would be predicted for <u>x</u> = (0 0 0 0 0)? What about for <u>x</u> = (1 1 0 1 0)?\n",
    "\n",
    "To prediction which class <u>x</u> = (0 0 0 0 0) belongs to, we calculate the probability of this data point belonging to each class y=1 and y=-1 and then compare both to check which has a higher probability. The data point will be predicted to belong to the class with the higher probability.\n",
    "\n",
    "Since, the features are conditionally independent, we can apply the product rule and chain rule to calculate to the probability of the data point belonging to each class as follows:\n",
    "\n",
    "P(x<sub>1</sub>=0, x<sub>2</sub>=0, x<sub>3</sub>=0, x<sub>4</sub>=0, x<sub>5</sub>=0, y=1) = P(x<sub>1</sub>=0, x<sub>2</sub>=0, x<sub>3</sub>=0, x<sub>4</sub>=0, x<sub>5</sub>=0  &#124; y=1)  x  P(y=1)\n",
    "= 0.009375\n",
    "\n",
    "P(x<sub>1</sub>=0, x<sub>2</sub>=0, x<sub>3</sub>=0, x<sub>4</sub>=0, x<sub>5</sub>=0, y=-1) = P(x<sub>1</sub>=0, x<sub>2</sub>=0, x<sub>3</sub>=0, x<sub>4</sub>=0, x<sub>5</sub>=0  &#124; y=-1)  x  P(y=-1)\n",
    "= 0.00185\n",
    "\n",
    "Now, since 0.009375 > 0.00185, it implies that the predicted class for <u>x</u> = (0 0 0 0 0) will be y=1.\n",
    "\n",
    "\n",
    "Similarly, for <u>x</u> = (1 1 0 1 0):\n",
    "\n",
    "P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0, y=1)  =  P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0  &#124; y=1)  x  P(y=1) = 0\n",
    "\n",
    "P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0, y=-1)  =   P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0  &#124; y=-1)  x  P(y=-1) = 0.07716\n",
    "\n",
    "Now since, 0 < 0.07716, we can conclude that the class predicted for <u>x</u> = (1 1 0 1 0) will be y=-1.\n",
    "\n",
    "\n",
    "##### 3. Computer the posterior probability that y = 1, given observation that  <u>x</u> = (1 1 0 1 0)\n",
    "\n",
    "According to the question, the posterior probability required can be formulated as:\n",
    "\n",
    "P(y=1 &#124; x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0)\n",
    "\n",
    "Now, according to Bayes Theorem, this is equal to:\n",
    "\n",
    "= P(y=1, x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0) /  P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0)  \n",
    "\n",
    "Now, from part 2. above, we know that P(x<sub>1</sub>=1, x<sub>2</sub>=1, x<sub>3</sub>=0, x<sub>4</sub>=1, x<sub>5</sub>=0, y=1) = 0.\n",
    "\n",
    "Therefore, the posterior probability that y=1 given <u>x</u> = (1 1 0 1 0) = 0.\n",
    "\n",
    "\n",
    "##### 4. Why should we probably not use a joint Bayes classifer (using the joint probability of the features x, as opposed to a naive Bayes classifer) for these data?\n",
    "\n",
    "We should probably not use a joint Bayes classifier as opposed to a naive Bayes classifier for this data because of the following reasons:\n",
    "\n",
    "- For a given no. of classes, for a joint Bayes classifier the no. of possible states increases exponentially with the no. of features whereas for a naive Bayes classifier this is only linear with the number of features. For example for a Binary classification problem (y=0 and y=-1), for 5 features, the no. of possible states will be 2^5 for a joint Bayes classifier whereas it will only be (2 x 5) x 2 = 20 for a Naive Bayes classifer. \n",
    "Therefore, it will be much more computationally intensive and impractical for computing a joint Bayes classifier as opposed to a Naive Bayes classifer.\n",
    "\n",
    "- Also, for the given scenario of email reading importance classification, the appearance of keywords can be assumed to be independent in this context to achieve good results.\n",
    "\n",
    "- Also, owing to the simplicity of the Naive Bayes classifier it is also immune to overfitting as opposed to the joint Bayes classifier.\n",
    "\n",
    "\n",
    "##### 5. Suppose that, before we make our predictions, we lose access to my address book, so that we cannot tell whether the email author is known. Should we re-train the model, and if so, how? (e.g.: how does the model, and its parameters, change in this new situation?) Hint: what will the naive Bayes model over only features look like, and what will its parameters be?\n",
    "\n",
    "No, we do not need to re-train our model in this case. This is because each feature is assumed to be conditionally independent and then the probabilities are calculated for each state combination for the Naive Bayes classifier. Which basically means that removing a feature due to data loss, will not affect the probabilities of other features, we will simply drop the factor corresponding to the removed feature. The input features will be 1 less. All other input probabilities P(x<sub>i</sub>=0 &#124; y=1), P(x<sub>i</sub>=1 &#124; y=1), P(x<sub>i</sub>=0 &#124; y=-1), P(x<sub>i</sub>=1 &#124; y=-1) will remain the same for features x<sub>i</sub> where i varies from 2 to 5. \n",
    "\n",
    "\n",
    "### Statement of Collaboration\n",
    "\n",
    "I completed this assignment on my own, without any collaboration with fellow students. I referred to discussions on Piazza to clear any doubts before starting on the project. After starting the assignment, I kept track of any clarifications made on Piazza to ensure nothing was missed by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
